---
title: "Course Project Analysis"
author: "pjpjean"
output: html_document
---

## Initializing
```{r initialize}
library(caret)
library(ggplot2)
library(randomForest)

wd <- function(relPath) {
  paste0("~/GitHub/PMLCourseProject", "/", relPath)
}
source(wd("code\\utils.R"))

draft <- FALSE

```

# Read datasets
```{r read-datasets, cache=TRUE}
pml <- read.csv(wd("data/pml-training.csv"),
                stringsAsFactors = FALSE,
                na.strings = c("","NA", "#DIV/0!"))
pml.submit <- read.csv(wd("data/pml-testing.csv"),
                     stringsAsFactors = FALSE,
                     na.strings = c("","NA", "#DIV/0!"))
```

# Examining variables
```{r examine-variables}
table(sapply(pml, class))
# check strings
head(pml[names(which(sapply(pml, class)=="character"))])
# check logicals
summary(pml[names(which(sapply(pml, class)=="logical"))])
# check NA's left
# remove all variables with more than 90% NA's
na.count <- sapply(pml, function(x) sum(is.na(x)))
na.count[na.count/nrow(pml) > .9]
summary(na.count)
hist(na.count)
hist(na.count[na.count/nrow(pml) > .9])
table(na.count)
table(na.count[na.count/nrow(pml) > .9])

# it seems that num_window is an id
table(pml$user_name, pml$num_window)
nrow(unique(pml[c("user_name", "num_window")]))
length(unique(pml$num_window))
```

# Some transformations
```{r transform}
# transform some variables
pml <- within(pml, {
  classe <- factor(classe)
  new_window <- as.numeric(new_window == "yes") # convert to logic (0 or 1)
})

pml.submit <- within(pml.submit, {
  # classe <- factor(classe, levels = levels(pml$classe))
  new_window <- as.numeric(new_window == "yes") # convert to logic (0 or 1)
})

table(sapply(pml, class))
head(pml)
```

this data would likely benefit of time series analysis since activities
have a logical connection (suggested variables: previous_activity, etc.)
Unfortunately, test set observations are isolated cases and thus have no
time series context (well, technically they have, because they have
timestamp, num_window, etc., but as they are isolated, they shouldn't)

user_name, otherwise, although is not informative as such, might
give us indirect information on age, height, weigth of our subjects.
user_name is a proxy for age, height, weigth ...

## Tidy dataset
```{r tidy-dataset}
# categorize variables
id.vars <- c("X", "num_window")
rem.vars <- c("raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp")
na.vars <- names(which(sapply(pml, function(x) sum(is.na(x))/length(x)) > 0.9))

y.var <- "classe"
x.vars <- names(pml)[!names(pml) %in% c(id.vars, rem.vars, na.vars, y.var)]
d.vars <- c(x.vars, y.var)
```

# Split dataset
We'll divide the dataset in two: training and test set.
training will be used to train the models and tuning will be done with cross-validation (or, in the case of random forest, with out-of-bag estimation)
test will be used to estimate each model's performance.

```{r}
set.seed(40187)
inTrain <- createDataPartition(pml$classe, p=0.7, list=FALSE)
pml.train <- pml[inTrain, d.vars]
pml.test <- pml[-inTrain, d.vars]
```
```{r, echo=FALSE}
if (draft) {
  pml.train <- pml.train[sample(nrow(pml.train), nrow(pml.train) * 0.1), ]
  pml.test <- pml.test[sample(nrow(pml.test), nrow(pml.test) * 0.1), ]
}
```

```{r exploratory}
print.cor(cor(classSubset(pml.train, c("numeric", "integer"))), min.coef = 0.6, lower.t = TRUE)
```

``` {r train-options}
if (draft) {
  default.trControl <- trainControl(method="cv", number = 10)
} else {
  default.trControl <- trainControl(method="repeatedcv", number=10, repeats=3)
}
```


```{r train-multinomial, cache=TRUE, results='hide'}
set.seed(43443)
mnom.trControl <- default.trControl
system.time(
  mnom.basemodel <- train(classe ~ ., 
                          data = pml.train, 
                          method = "multinom", 
                          trControl = mnom.trControl,
                          tuneLength = 5)
  )

mnom.basemodel
mnom.basemodel$finalModel

```

```{r train-multinomial-2, cache=TRUE, results='hide'}
num.vars <- classVars(pml.train, c("numeric", "integer"))
cor.vars <- num.vars[findCorrelation(
  cor(classSubset(pml.train, c("numeric", "integer"))), 
  cutoff = .75)]
lowcor.vars <- names(pml.train)[!d.vars %in% cor.vars]

system.time(
  mnom.lowcormodel <- train(classe ~ ., 
                          data = pml.train[,lowcor.vars], 
                          method = "multinom", 
                          trControl = mnom.trControl,
                          tuneLength = 5)
  )

mnom.lowcormodel
mnom.lowcormodel$finalModel

```

## random forest
In random forests, there is no need for cross-validation or a separate test
set to get an unbiased estimate of the test set error. It is estimated
internally, during the run, as follows:
```{r train-randomforest, cache=TRUE, results='hide'}
set.seed(43443)
rf.trControl <- trainControl(method="oob")

system.time(
  rf.basemodel <- train(classe ~ ., 
                        data = pml.train, 
                        method = "rf", 
                        trControl = rf.trControl,
                        tuneLength = 5)
)
```


``` {r}
rf.basemodel
rf.basemodel$finalModel

#pred.basemodel <- predict(rf.basemodel,newdata = pml.validation)
#confusionMatrix(pml.validation$classe, pred.basemodel)
varImp(rf.basemodel)

pml_write_files(as.character(predict(rf.basemodel, newdata=pml.submit)),
                "rf-mtry_16")
```

```{r train-svmRadialCost, cache=TRUE, results='hide'}
set.seed(43443)
svm.trControl <- default.trControl
system.time(
  svm.basemodel <- train(classe ~ ., 
                         data = pml.train, 
                         method = "svmRadialCost",
                         preProc = c("center", "scale"),
                         trControl = svm.trControl,
                         tuneLength = 9)
)
```


``` {r}
svm.basemodel
svm.basemodel$finalModel
