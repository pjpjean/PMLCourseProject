---
title: "Course Project Analysis"
author: "pjpjean"
output: html_document
---
```{r setup, echo=FALSE}
opts_chunk$set(tidy=TRUE, tidy.opts=list(arrow=TRUE, blank=FALSE))

# get the default output hook
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    hook_output(x, options) # pass to default hook
  }
  else {
    x <- unlist(stringr::str_split(x, "\n"))
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), "...\n")
    }
    # paste these lines together
    x <- paste(x, collapse = "\n")
    hook_output(x, options)
  }
})
```

# Initializing
```{r initialize, message=FALSE, warning=FALSE, echo=-4}
library(caret)
library(ggplot2)
library(randomForest)

wd <- function(relPath) {
  paste0("~/Pessoais/Coursera/PMLCourseProject", "/", relPath)
}
source(wd("code\\utils.R"))
```

```{r draft-flag, echo=FALSE}
draft <- FALSE
```

# Read datasets
```{r read-datasets, cache=TRUE}
pml <- read.csv(wd("data/pml-training.csv"),
                stringsAsFactors = FALSE,
                na.strings = c("","NA", "#DIV/0!"))
pml.submit <- read.csv(wd("data/pml-testing.csv"),
                     stringsAsFactors = FALSE,
                     na.strings = c("","NA", "#DIV/0!"))
```

# Examining variables
```{r examine-variables, output.lines = 24}
table(sapply(pml, class))
# check strings
head(classSubset(pml, "character"))
# check logicals
summary(classSubset(pml, "logical"))
# will remove all variables with more than 90% NA's
na.count <- sapply(pml, function(x) sum(is.na(x)))
na.count[na.count/nrow(pml) > .9]
summary(na.count)
hist(na.count)
hist(na.count[na.count/nrow(pml) > .9])
table(na.count)
table(na.count[na.count/nrow(pml) > .9])

# it seems that num_window is an id
table(pml$user_name, pml$num_window)
nrow(unique(pml[c("user_name", "num_window")]))
length(unique(pml$num_window))
```

# Some transformations
```{r transform}
# transform some variables
pml <- within(pml, {
  classe <- factor(classe)
  new_window <- as.numeric(new_window == "yes") # convert to logic (0 or 1)
})

pml.submit <- within(pml.submit, {
  # classe <- factor(classe, levels = levels(pml$classe))
  new_window <- as.numeric(new_window == "yes") # convert to logic (0 or 1)
})

table(sapply(pml, class))
#head(pml, 1)
```

this data would likely benefit of time series analysis since activities
have a logical connection (suggested variables: previous_activity, etc.)
Unfortunately, test set observations are isolated cases and thus have no
time series context (well, technically they have, because they have
timestamp, num_window, etc., but as they are isolated, they shouldn't)

user_name, otherwise, although is not informative as such, might
give us indirect information on age, height, weigth of our subjects.
user_name is a proxy for age, height, weigth ...

## Tidy dataset
```{r tidy-dataset}
# categorize variables
id.vars <- c("X", "num_window")
rem.vars <- c("raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp")
na.vars <- names(which(sapply(pml, function(x) sum(is.na(x))/length(x)) > 0.9))

y.var <- "classe"
x.vars <- names(pml)[!names(pml) %in% c(id.vars, rem.vars, na.vars, y.var)]
d.vars <- c(x.vars, y.var)
```

# Split dataset
We'll divide the dataset in two: training and test set.
training will be used to train the models and tuning will be done with cross-validation (or, in the case of random forest, with out-of-bag estimation)
test will be used to estimate each model's performance.

```{r}
set.seed(40187)
inTrain <- createDataPartition(pml$classe, p=0.7, list=FALSE)
pml.train <- pml[inTrain, d.vars]
pml.test <- pml[-inTrain, d.vars]
```
```{r, echo=FALSE, dependson='draft-flag'}
if (draft) {
  pml.train <- pml.train[sample(nrow(pml.train), nrow(pml.train) * 0.1), ]
  pml.test <- pml.test[sample(nrow(pml.test), nrow(pml.test) * 0.1), ]
}
```

```{r exploratory}
print.cor(cor(classSubset(pml.train, c("numeric", "integer"))), min.coef = 0.6, lower.t = TRUE)
```

``` {r train-options, dependson='draft-flag'}
if (draft) {
  default.trControl <- trainControl(method="cv", number = 10)
} else {
  default.trControl <- trainControl(method="repeatedcv", number=10, repeats=3)
}
```

```{r train-multinomial, cache=TRUE, results='hide', message=FALSE, warning=FALSE, dependson='draft-flag'}
set.seed(43443)
mnom.trControl <- default.trControl
mnom.time <- system.time(
  mnom.basemodel <- train(classe ~ ., 
                          data = pml.train, 
                          method = "multinom", 
                          trControl = mnom.trControl,
                          tuneLength = 5)
)
```

```{r results-multinomial}
mnom.time
mnom.basemodel
t(coef(mnom.basemodel$finalModel))
```

```{r train-multinomial-2, cache=TRUE, results='hide', message=FALSE, warning=FALSE, dependson='draft-flag'}
num.vars <- classVars(pml.train, c("numeric", "integer"))
cor.vars <- num.vars[findCorrelation(
  cor(classSubset(pml.train, c("numeric", "integer"))), 
  cutoff = .75)]
lowcor.vars <- names(pml.train)[!d.vars %in% cor.vars]

mnom.lowcortime <- system.time(
  mnom.lowcormodel <- train(classe ~ ., 
                          data = pml.train[,lowcor.vars], 
                          method = "multinom", 
                          trControl = mnom.trControl,
                          tuneLength = 5)
)
```

```{r results-multinomial-2}
mnom.lowcortime
mnom.lowcormodel
mnom.lowcormodel$finalModel
```

## random forest
In random forests, there is no need for cross-validation or a separate test
set to get an unbiased estimate of the test set error. It is estimated
internally, during the run, as follows:

```{r train-randomforest, cache=TRUE, results='hide', dependson='draft-flag'}
set.seed(43443)
rf.trControl <- trainControl(method="oob")

rf.time <- system.time(
  rf.basemodel <- train(classe ~ ., 
                        data = pml.train, 
                        method = "rf", 
                        trControl = rf.trControl,
                        tuneLength = 5)
)
```

``` {r results-randomforest}
rf.time
rf.basemodel
rf.basemodel$finalModel

#pred.basemodel <- predict(rf.basemodel,newdata = pml.validation)
#confusionMatrix(pml.validation$classe, pred.basemodel)
varImp(rf.basemodel)

#pml_write_files(as.character(predict(rf.basemodel, newdata=pml.submit)),
#                "rf-mtry_16")
```

```{r train-svmRadialCost, cache=TRUE, results='hide', message=FALSE, warning=FALSE, dependson='draft-flag'}
set.seed(43443)
svm.trControl <- default.trControl
svm.time <- system.time(
  svm.basemodel <- train(classe ~ ., 
                         data = pml.train, 
                         method = "svmRadialCost",
                         preProc = c("center", "scale"),
                         trControl = svm.trControl,
                         tuneLength = 9)
)
```

``` {r results-svmRadialCost, message=FALSE, warning=FALSE}
svm.time
svm.basemodel
svm.basemodel$finalModel
```

```{r predictions, message=FALSE, warning=FALSE}
sel.models <- list(mnom = mnom.basemodel, 
                   mnom.lc = mnom.lowcormodel, 
                   rf = rf.basemodel, 
                   svm = svm.basemodel)
test.pred <- predict(sel.models, newdata = pml.test)
t(sapply(test.pred, pml.test$classe, FUN=function(pred, obs) {
  cm <- confusionMatrix(pred, obs)
  round(cm$overall[1:4] * 100, 1)
  }))
```

